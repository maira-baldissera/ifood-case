{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eec0ff",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ccd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializa sessão Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define diretório target e URL\n",
    "url = \"https://data-architect-test-source.s3.sa-east-1.amazonaws.com/ds-technical-evaluation-data.tar.gz\"\n",
    "raw_dir = os.path.join(os.getcwd().split('\\\\notebooks')[0], \"data\\\\raw\")\n",
    "processed_dir = os.path.join(os.getcwd().split('\\\\notebooks')[0], \"data\\\\processed\")\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "# Carrega o arquivo .tar.gz \n",
    "local_tar_path = os.path.join(raw_dir, \"data_archive.tar.gz\")\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(local_tar_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Carregado para: {local_tar_path}\")\n",
    "else:\n",
    "    raise Exception(f\"Download falhou com código de status: {response.status_code}\")\n",
    "\n",
    "# Step 3: Extrai na pasta raw\n",
    "try:\n",
    "    with tarfile.open(local_tar_path, mode=\"r:*\") as tar:\n",
    "        tar.extractall(path=raw_dir)\n",
    "    print(f\"Arquivos extraídos para: {raw_dir}\")\n",
    "except tarfile.ReadError as e:\n",
    "    raise Exception(\"Extração falhou: não é um arquivo tar válido\") from e\n",
    "\n",
    "# Step 4: Lista arquivos extraídos\n",
    "print(\"Arquivos extraídos:\")\n",
    "for root, dirs, files in os.walk(raw_dir):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "# Step 4: Dynamically find the JSON file paths\n",
    "offer_path = ''\n",
    "customer_path = ''\n",
    "transaction_path = ''\n",
    "\n",
    "lista_nome_arquivos = ['ds-technical-evaluation-data\\\\offers.json', \n",
    "                        'ds-technical-evaluation-data\\\\profile.json', \n",
    "                        'ds-technical-evaluation-data\\\\transactions.json']\n",
    "extract_path = raw_dir  \n",
    "\n",
    "for nome in lista_nome_arquivos:\n",
    "    if nome == 'ds-technical-evaluation-data\\\\offers.json':\n",
    "        offer_path = os.path.join(extract_path, nome)\n",
    "    elif nome == 'ds-technical-evaluation-data\\\\profile.json':\n",
    "        customer_path = os.path.join(extract_path, nome)\n",
    "    elif nome == 'ds-technical-evaluation-data\\\\transactions.json':\n",
    "        transaction_path = os.path.join(extract_path, nome)\n",
    "\n",
    "# Ensure all files were found\n",
    "\n",
    "\n",
    "print(f'OFFERPATH:{offer_path}')\n",
    "print(f'CUSTOMERPATH:{customer_path}')\n",
    "print(f'TRANSACTIONPATH:{transaction_path}')\n",
    "#raise FileNotFoundError(\"Um ou mais arquivos JSON não foram encontrados após extração.\")\n",
    "\n",
    "try:\n",
    "    # whatever block of code may be failing\n",
    "    df_offers = spark.read.option(\"multiline\", True).json(f\"file:///{offer_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Original error:\", str(e))\n",
    "\n",
    "try:\n",
    "    # whatever block of code may be failing\n",
    "    df_customers = spark.read.option(\"multiline\", True).json(f\"file:///{customer_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Original error:\", str(e))\n",
    "\n",
    "try:\n",
    "    # whatever block of code may be failing\n",
    "    df_transactions = spark.read.option(\"multiline\", True).json(f\"file:///{transaction_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Original error:\", str(e))\n",
    "\n",
    "\n",
    "# Step 6: Preview\n",
    "print(\"=== OFFERS ===\")\n",
    "df_offers.show(truncate=False)\n",
    "\n",
    "print(\"=== CUSTOMERS ===\")\n",
    "df_customers.show(truncate=False)\n",
    "\n",
    "print(\"=== TRANSACTIONS ===\")\n",
    "df_transactions.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2f3a7",
   "metadata": {},
   "source": [
    "Filtrando offers e salvando "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, lower\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "\n",
    "# Remover registros com ID nulo ou vazio\n",
    "df_offers= df_offers.filter((col(\"id\").isNotNull()) & (col(\"id\") != \"\"))\n",
    "\n",
    "# Verificar e tratar valores inconsistentes nos campos categóricos\n",
    "valid_types = [\"bogo\", \"discount\", \"informational\"]\n",
    "df_offers = df_offers.withColumn(\"offer_type\", col(\"offer_type\").cast(\"string\"))\n",
    "df_offers = df_offers.filter(col(\"offer_type\").isin(valid_types))\n",
    "\n",
    "# Preencher valores nulos ou ausentes\n",
    "df_offers = df_offers.fillna({\n",
    "    \"duration\": 0,\n",
    "    \"discount_value\": 0,\n",
    "    \"min_value\": 0\n",
    "})\n",
    "\n",
    "# Validar que canais existam (lista não vazia)\n",
    "df_offers = df_offers.filter((col(\"channels\").isNotNull()) & (size(col(\"channels\")) > 0))\n",
    "\n",
    "# Ajustar campos de texto para lowercase (padronização)\n",
    "df_offers = df_offers.withColumn(\"offer_type\", lower(col(\"offer_type\")))\n",
    "\n",
    "#Salvar o DataFrame tratado\n",
    "df_offers.coalesce(1).write.mode(\"overwrite\").json(os.path.join(processed_dir, \"offers_cleaned\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce730f96",
   "metadata": {},
   "source": [
    "Filtrando customers e salvando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, trim\n",
    "\n",
    "# Remover registros com campos essenciais nulos\n",
    "df_customers = df_customers.filter(\n",
    "    (col(\"id\").isNotNull()) &\n",
    "    (col(\"age\").isNotNull()) &\n",
    "    (col(\"credit_card_limit\").isNotNull()) &\n",
    "    (col(\"registered_on\").isNotNull())\n",
    ")\n",
    "\n",
    "# Corrigir e padronizar campo \"gender\"\n",
    "# a) Remover espaços extras\n",
    "df_customers = df_customers.withColumn(\"gender\", trim(col(\"gender\")))\n",
    "\n",
    "# b) Manter apenas os gêneros válidos\n",
    "valid_genders = [\"M\", \"F\", \"O\"]\n",
    "df_customers = df_customers.filter((col(\"gender\").isin(valid_genders)) & (col(\"gender\").isNotNull()))\n",
    "\n",
    "# Filtrar idades válidas\n",
    "df_customers = df_customers.filter((col(\"age\") >= 10) & (col(\"age\") <= 100) & (col(\"age\").isNotNull()))\n",
    "\n",
    "# Converter `registered_on` para tipo data\n",
    "df_customers = df_customers.withColumn(\n",
    "    \"registered_on\", to_date(col(\"registered_on\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Renomear `id` para `customer_id` (boa prática para joins)\n",
    "df_customers = df_customers.withColumnRenamed(\"id\", \"customer_id\")\n",
    "\n",
    "# Salvar o DataFrame tratado\n",
    "df_customers.coalesce(1).write.mode(\"overwrite\").json(os.path.join(processed_dir, \"customers_cleaned\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546893b",
   "metadata": {},
   "source": [
    "Filtrando transactions e salvando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, when, struct\n",
    "\n",
    "# 1. Remover registros sem account_id ou event\n",
    "df_transactions = df_transactions.filter(\n",
    "    (col(\"account_id\").isNotNull()) &\n",
    "    (col(\"event\").isNotNull())\n",
    ")\n",
    "# Desmontar a tupla\n",
    "df_transactions = df_transactions.withColumn(\"amount\", col(\"value.amount\")) \\\n",
    "       .withColumn(\"offer id\", col(\"value.offer id\")) \\\n",
    "       .withColumn(\"offer_id\", col(\"value.offer_id\")) \\\n",
    "       .withColumn(\"reward\", col(\"value.reward\"))\n",
    "df_trasactions = df_transactions.drop(\"value\")\n",
    "\n",
    "# Padronizar os eventos em lowercase e sem espaços extras\n",
    "df_transactions = df_transactions.withColumn(\"event\", trim(col(\"event\")))\n",
    "\n",
    "# Unir coluna offer id e offer_id para valores não nulos\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"offer_id\",\n",
    "    when(col(\"offer_id\").isNotNull(), col(\"offer_id\"))\n",
    "    .otherwise(col(\"offer id\"))\n",
    ")\n",
    "\n",
    "# Remover a coluna redundante com espaço\n",
    "df_transactions = df_transactions.drop(\"offer id\")\n",
    "\n",
    "# Preencher campos numéricos ausentes com 0 (opcional e seguro se reward e amount forem esparsos)\n",
    "df_transactions = df_transactions.fillna({\n",
    "    \"amount\": 0.0,\n",
    "    \"reward\": 0.0\n",
    "})\n",
    "\n",
    "# Validar tipos dos eventos conhecidos\n",
    "df_transactions.select(\"event\").distinct().show()\n",
    "valid_events = [\"offer received\", \"offer viewed\", \"offer completed\", \"transaction\"]\n",
    "df_transactions = df_transactions.filter(col(\"event\").isin(valid_events))\n",
    "\n",
    "# Salvar o DataFrame tratado\n",
    "df_transactions.coalesce(1).write.mode(\"overwrite\").json(os.path.join(processed_dir, \"transactions_cleaned\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
